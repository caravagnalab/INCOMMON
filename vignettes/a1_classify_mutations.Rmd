---
title: "2. Inference of copy number and mutation multiplicity"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{1. Inference of copy number and mutation multiplicity}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

options(crayon.enabled=TRUE)
```

```{r setup}
library(INCOMMON)
library(dplyr)
```

In this vignette, we classify mutations from a single sample of the MSK-MetTropsim dataset 
provided within the package. We will look at the classification entropy and study the effect of
a cut-off on this quantity. In addition, we will see how using the prior obtained from PCAWG
whole genomes can help the classification of low quality data.

```{r}
sample = 'P-0002081'

genomic_data = MSK_genomic_data %>% filter(sample == !!sample)
clinical_data = MSK_clinical_data %>% filter(sample == !!sample)

print(genomic_data)
print(clinical_data)
```
By inspecting the clinical data table, we can see that this is a sample of a metastatic lung adenocarcinoma (LUAD), with purity 0.6, sequenced through the MSK-IMPACT targeted panel version 341.

The genomic data table contains 4 mutations affecting KRAS, TP53, STK11 and SMARCA4 genes.

## Input preparation

First we initialise the input in INCOMMON format through function `init`, using
the gene roles from the COSMIC Cancer Gene Census v.98 provided within the package.


```{r}
x = init(genomic_data = genomic_data, 
         clinical_data = clinical_data, 
         gene_roles = INCOMMON::cancer_gene_census)

print(x)
```

All the requirements for INCOMMON classification are satisfied. The average sequencing
depth is 380.

## Classification without prior knowledge and entropy cut-off.

We now run the classification step through function `classify`. Here, we do not
use any prior knowledge (`priors` set to `NULL`) nor a cut-off on the entropy
(`entropy_cutoff` set to 1).

```{r, message=FALSE}
x = classify(x = x, 
             priors = NULL, 
             entropy_cutoff = 1)
```

```{r}
print(x)
```

There are $N=4$ mutations with amplification (AM) and no mutation in any other state.
The entropy though is quite high, with an average of $\langle  H(x)\rangle = 0.26$ and
a maximum of $\max{H(x)} = 0.34$. 

```{r}
classification(x) %>% 
  ggplot2::ggplot(ggplot2::aes(x = DP, y = entropy, color = state))+
  ggplot2::geom_point(stat = 'identity')+ggplot2::theme_bw()+
  INCOMMON:::scale_color_INCOMMON_class(aes = 'color')
```

It is evident that, given the sample purity $\pi = 0.6$, mutations with lower 
sequencing depth have higher entropy, because they are harder to classify.

## Visualising INCOMMON classification 

INCOMMON allows to visualise a representation of the maximum a posteriori
classification through function `plot_classification`. Let us can compare the results
between low-depth/high-entropy and high-depth/low-entropy classified mutation:

```{r}
plot_classification(x, sample = sample, assembly = T)
```

Differently from the first mutation, the low sequencing depth of the others shrink
the domain of the Beta-Binomial mixture to a narrower region, where the components 
largely overlap. The different entropy curves (magenta line) reflect the increase in
classification uncertainty for mutations with low sequencing depth.

## Improving classifications with PCAWG priors

To improve the classification of poorly supported mutations, we include informative
priors in the model. INCOMMONS provide a set of prior distributions (gene-specific
pan-cancer or gene-tumor specific, if available) obtained from validated CNA calls
of driver mutations in PCAWG whole-genomes.

Given the advantage of using prior knowledge, we can decide to be more restrictive
about classification entropy, by setting an entropy cut-off. The default value
is 0.2, which seems to be reasonable also for this particular sample.

```{r}
x = classify(x = x, 
             priors = pcawg_priors, 
             entropy_cutoff = 0.2)

print(x)
```
There are no tumor-specific prior distributions for the mutant genes in this sample.
However, a pan-cancer prior is used. We can see already some difference in the results:
there are now 3 mutations in LOH and only one with amplification. Moreover, the mean entropy
is now 0.04 and the maximum is 0.07! 

No mutation was classified as Tier-2, meaning that in all cases the classification
entropy was lower than the chosen cutoff.

Let's have a look at how the maximum a posteriori classification has changed:

```{r}
plot_classification(x, sample = sample, assembly = T)
```

Even if the overlapping of mixture components is still high, some of them have
evidently lower weight, yielding higher confidence (i.e. lower entropy) in the 
maximum a posteriori estimate.

## Visualising prior distributions

To understand what has changed in the mixture model after using the priors, let 
have a look at them. For example, without any prior the mutation on TP53 was classified as amplified (AM), with high overlapping between
the components corresponding to AM and LOH states. When using the prior, LOH
states are given more weight and the mutations is classified as LOH.

What is the prior distribution on TP53 in LUAD tumors? We can use the function
`plot_prior` to visualise it:

```{r}
plot_prior(x = pcawg_priors, gene = 'TP53', tumor_type = 'LUAD')
```

There is no prior distribution specific for lung adenocarcinoma, and so the used pan-cancer prior is visualised. LOH and CNLOH states have clearly higher frequency than AM, and this is the reason for the different weights that 
the corresponding components have in the model posterior.